# Shared base-path configuration
APP_BASE_PATH=/quiz-me
VITE_APP_BASE_PATH=/quiz-me/
VITE_API_PROXY_TARGET=http://127.0.0.1:5000

# CORS and request limits
CORS_ORIGINS=https://apps.aniketshedge.com
MAX_CONTENT_LENGTH_MB=2
MAX_REQ_PER_10MIN=60
MAX_QUIZ_CREATIONS_PER_10MIN=5

# LLM provider order and fallback
LLM_PROVIDER_1=openai
LLM_PROVIDER_2=perplexity
LLM_PROVIDER_3=gemini
LLM_TIMEOUT_MS=90000
LLM_MAX_RETRIES_PER_PROVIDER=0
# Use "all" to attempt failover for any provider/category error.
LLM_FAILOVER_ON=all
LLM_ALLOW_MOCK=true
# When true, disables all LLM calls and forces deterministic mock quiz/grading flow.
LLM_FORCE_MOCK_MODE=false
LLM_TELEMETRY_ENABLED=true
LLM_TELEMETRY_DIR=runtime/llm_telemetry

# Generic task model defaults
MODEL_TOPIC_GUARDRAIL=gpt-5-nano
MODEL_QUIZ_GENERATION=gpt-5-mini
MODEL_SHORT_GRADING=gpt-5-mini

# Provider-specific model overrides (optional)
OPENAI_MODEL_TOPIC_GUARDRAIL=gpt-5-nano
OPENAI_MODEL_QUIZ_GENERATION=gpt-5-mini
OPENAI_MODEL_SHORT_GRADING=gpt-5-mini

PERPLEXITY_MODEL_TOPIC_GUARDRAIL=sonar
PERPLEXITY_MODEL_QUIZ_GENERATION=sonar
PERPLEXITY_MODEL_SHORT_GRADING=sonar

GEMINI_MODEL_TOPIC_GUARDRAIL=gemini-2.5-flash-lite
GEMINI_MODEL_QUIZ_GENERATION=gemini-2.5-flash
GEMINI_MODEL_SHORT_GRADING=gemini-2.5-flash-lite

# API keys and base URLs
OPENAI_API_KEY=
OPENAI_BASE_URL=https://api.openai.com/v1

PERPLEXITY_API_KEY=
PERPLEXITY_BASE_URL=https://api.perplexity.ai

GEMINI_API_KEY=
GEMINI_BASE_URL=https://generativelanguage.googleapis.com/v1beta

# Wikipedia retrieval and grading
WIKI_LANG=en
WIKI_MAX_CHARS=24000
WIKI_SUMMARY_TARGET_CHARS=8000
WIKI_USER_AGENT=quiz-me-app/0.1 (https://apps.aniketshedge.com/quiz-me/; quiz-me-demo)
SHORT_GRADE_CONFIDENCE_THRESHOLD=0.60

# Frontend API timeout
VITE_API_TIMEOUT_MS=120000
